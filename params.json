{
  "name": "Image-classification",
  "tagline": "Some work on the CIFAR-10 dataset. PCA, LDA, Random Forest, K-Nearest Neighbor",
  "body": "#Description\r\n\r\nThis page is a short overview of my work on the CIFAR-10 dataset using 10-fold cross validation and PCA, LDA, Random Forest Classification and K-Nearest Neighbor Classification.\r\n\r\n#Data Processing\r\n\r\n\r\nTo start, we have the need to prepare the data by creating test and training data sets. Once we have that, we can start classifying.\r\n\r\n\r\n```python\r\n#code to prepare data for classification, create test and training sets\r\nimport pandas as pd\r\nfrom sklearn.cross_validation import train_test_split\r\n\r\ndata = pd.read_csv('train1p_with_header.csv')\r\nX=data.iloc[:,0:data.shape[1]-1].as_matrix()\r\ny=data.iloc[:,data.shape[1]-1]\r\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) \r\n```\r\n\r\n\r\n#Classifiers\r\n\r\n\r\nThe first classifier I completed was a PCA (Principle Component Analysis) to Random Forest Classifier pipe. This approach resulted in a classifier accuracy of 32%.\r\n\r\n\r\n```python\r\n#pca->rfc classifier\r\nimport numpy as np\r\nfrom sklearn.decomposition import PCA\r\nfrom sklearn.ensemble import RandomForestClassifier\r\n\r\ndef pca_rfc(X_train,X_test,y_train,y_test):\r\n    pca = PCA(n_components=500)\r\n    pca.fit(X_train)\r\n    scores = np.dot(X_train,np.transpose(pca.components_))\r\n    rfc = RandomForestClassifier(n_estimators=500, oob_score=True, random_state = 60)\r\n    rfc.fit(scores,y_train)\r\n    return rfc.oob_score_\r\n\r\npca_rfc(X_train,X_test,y_train,y_test)\r\n```\r\n\r\n\r\nThe second classifier I completed was a PCA (Principle Component Analysis) to LDA (Linear Discriminant Analysis) pipe. This approach resulted in a classifier accuracy of 58%.\r\n\r\n\r\n```python\r\n#pca->lda classifier\r\nimport numpy as np\r\nfrom sklearn.decomposition import PCA\r\nfrom sklearn.lda import LDA\r\n\r\ndef pca_lda(X_train,X_test,y_train,y_test):\r\n    pca = PCA(n_components=500)\r\n    lda = LDA()\r\n    pca.fit(X_train)\r\n    scores = np.dot(X_train,np.transpose(pca.components_))\r\n    lda.fit(scores, y_train)\r\n    return lda.score(scores, y_train, sample_weight=None)\r\n\r\npca_lda(X_train,X_test,y_train,y_test)\r\n```\r\n\r\n\r\nThe third classifier I completed was a PCA (Principle Component Analysis) to KNN (K-Nearest-Neighbor) pipe. This approach resulted in a classifier accuracy of 38%.\r\n\r\n\r\n```python\r\n#pca->knn classifier\r\nimport numpy as np\r\nfrom sklearn.decomposition import PCA\r\nfrom sklearn.neighbors import KNeighborsClassifier\r\n\r\ndef pca_knn(X_train,X_test,y_train,y_test):\r\n    pca = PCA(n_components=500)\r\n    pca.fit(X_train)\r\n    scores = np.dot(X_train,np.transpose(pca.components_))\r\n    knn = KNeighborsClassifier(10)\r\n    knn.fit(scores,y_train)\r\n    return knn.score(scores, y_train)\r\n    \r\npca_knn(X_train,X_test,y_train,y_test)\r\n```\r\n\r\n\r\nLastly, I completed a 10-fold cross-validation that can be used for any of the classifiers. In the example below, I am running a 10-fold cross-validation on the PCA to Random Forest Classification. This cross-validation resulted in an accuracy of 32%.\r\n\r\n\r\n```python\r\n#10 fold cross validation\r\nfrom sklearn.cross_validation import KFold\r\nimport numpy as np\r\n\r\ntenKFold = KFold(len(X[:,0]), 10)\r\naccuracy = np.zeros(10)\r\ni=0\r\nfor train_index, test_index in tenKFold:\r\n    X_train, X_test = X[train_index], X[test_index]\r\n    y_train, y_test = y[train_index], y[test_index]\r\n    accuracy[i]= pca_rfc(X_train,X_test,y_train,y_test)\r\n    print accuracy[i]\r\n    i+=1\r\nprint(sum(accuracy)/10)\r\n```\r\n\r\n\r\n\r\n#RShiny Application \r\n\r\nOnce I was able to get the classifications and cross validation working, I created an RShiny application to visualize some of the data. I decided to create boxplots for each classifier's cross validation data along with a data plot. I also created a boxplot comparing the three classifier's cross validations, which will definitely be more useful when the entirety of the data is tested/when the classifiers are trained to higher accuracies.\r\n\r\n\r\n\r\n\r\nThe code below is for the user interface (ui.r), which handles interactions with the user and passes them to the server.\r\n\r\n```R\r\nlibrary(shiny)\r\nshinyUI(fluidPage(\r\n  \r\n  titlePanel(\"Image Classification\"),\r\n  \r\n  sidebarLayout(\r\n    sidebarPanel(\r\n      selectInput('class', 'Classifier', list(\"PCA -> Random Forest Classification\" = \"RFC\", \r\n                                              \"PCA -> Linear Discriminant Analysis\" = \"LDA\", \r\n                                              \"PCA -> K-Nearest Neighbor\" = \"KNN\"))\r\n    ),\r\n    \r\n    mainPanel(\r\n      tabsetPanel(\r\n        tabPanel(\"Box Plot\",\r\n                 plotOutput(\"plot1\"),\r\n                 h4(\"Five Number Summary: MIN, Q1, Q2, Q3, MAX\"),\r\n                 textOutput(\"getStats\")\r\n        ),\r\n        tabPanel(\"Dot Plot\",\r\n                 plotOutput(\"plot2\")\r\n        ),\r\n        tabPanel(\"Box Plot Comparison\",\r\n                 plotOutput(\"plot3\")\r\n        )     \r\n      )\r\n      \r\n    )\r\n  )\r\n))\r\n```\r\n\r\n\r\n\r\n\r\nThe code below is for the server side of the application (server.r), which handles all of the plot creation and data handling.\r\n\r\n```R\r\nlibrary(shiny)\r\ncvData <- read.csv(file = \"/home/spring2016_csci334user13/img_app/cvData.csv\")\r\n\r\nshinyServer(function(input, output) {\r\n\r\n  output$plot1 <- renderPlot({\r\n    name = input$class\r\n    \r\n    if (name==\"RFC\") {\r\n      boxplot(cvData$rfc)\r\n    } else if (name==\"LDA\") {\r\n      boxplot(cvData$lda)\r\n    } else {\r\n      boxplot(cvData$knn)\r\n    }\r\n    \r\n  })\r\n  \r\n  output$plot2 <- renderPlot({\r\n    name = input$class\r\n    \r\n    if (name==\"RFC\") {\r\n      y <- cvData[,1]\r\n      name = \"PCA -> Random Forest Classification\"\r\n    } else if (name==\"LDA\") {\r\n      y <- cvData[,2]\r\n      name = \"PCA -> Linear Discriminant Analysis\"\r\n    } else {\r\n      y <- cvData[,3]\r\n      name = \"PCA -> K-Nearest Neighbor\"\r\n    }\r\n    x <- 1:10\r\n\r\n    plot(x,y,main=paste(\"10-Fold Cross Validation for\", name),xlab=\"Cross-Validation Iteration\",\r\n         ylab=\"Accuracy\",ylim=c(0,1),col = 'black')\r\n  })\r\n  \r\n  output$plot3 <- renderPlot({\r\n    boxplot(cvData, main=\"Accuracy by Classification\")  \r\n  })\r\n  \r\n  output$getStats <- renderText({\r\n    \r\n    name = input$class\r\n    \r\n    if (name==\"RFC\") { \r\n      rfcPlot <- boxplot(cvData$rfc)\r\n      paste(rfcPlot$stats)\r\n\r\n    } else if (name==\"LDA\") {\r\n      ldaPlot <- boxplot(cvData$lda)\r\n      paste(ldaPlot$stats)\r\n    } else {\r\n      knnPlot <- boxplot(cvData$knn)\r\n      paste(knnPlot$stats)\r\n    }\r\n    \r\n  })\r\n  \r\n})\r\n```\r\n\r\n\r\n\r\n\r\nNext, we'll take a look at some of the screenshots from the application being used.\r\n\r\n\r\n\r\n\r\n###PCA->Random Forest Classification\r\n<a href=\"http://imgur.com/L2a7v7l\"><img src=\"http://i.imgur.com/L2a7v7l.png\" title=\"source: imgur.com\" /></a>\r\n<a href=\"http://imgur.com/Mxnq0Vo\"><img src=\"http://i.imgur.com/Mxnq0Vo.png\" title=\"source: imgur.com\" /></a>\r\n\r\n\r\n\r\n###PCA->LDA Classification\r\n<a href=\"http://imgur.com/4MVoefC\"><img src=\"http://i.imgur.com/4MVoefC.png\" title=\"source: imgur.com\" /></a>\r\n<a href=\"http://imgur.com/JOfiRwr\"><img src=\"http://i.imgur.com/JOfiRwr.png\" title=\"source: imgur.com\" /></a>\r\n\r\n\r\n\r\n###PCA->KNN Classification\r\n<a href=\"http://imgur.com/K2MRFrg\"><img src=\"http://i.imgur.com/K2MRFrg.png\" title=\"source: imgur.com\" /></a>\r\n<a href=\"http://imgur.com/0o0o19s\"><img src=\"http://i.imgur.com/0o0o19s.png\" title=\"source: imgur.com\" /></a>\r\n\r\n\r\n\r\n###Boxplot of All Three Classifiers' Cross Validation Data\r\n<a href=\"http://imgur.com/GD9NIf3\"><img src=\"http://i.imgur.com/GD9NIf3.png\" title=\"source: imgur.com\" /></a>\r\n\r\n\r\n\r\n#Conclusion\r\n\r\nIn conclusion, I enjoyed the project and thought that it was pretty fun. I had never used RShiny or scikit learn before so it was a good experience. I may download the full dataset and set up a local environment so I can keep training the classifiers.\r\n",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}